{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the setup instructions in `setup.md`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Blaizzy/mlx-vlm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install --upgrade mlx-vlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from mlx_vlm import load, generate\n",
    "from mlx_vlm.prompt_utils import apply_chat_template\n",
    "from mlx_vlm.utils import load_config\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_path = \"mlx-community/Qwen2.5-VL-3B-Instruct-4bit\"\n",
    "model, processor = load(model_path)\n",
    "config = load_config(model_path)\n",
    "\n",
    "# Prepare input\n",
    "image_url = \"https://drive.google.com/uc?id=1ZCA4qnbti8SKuosUtUBCnCwWEezUiA5B\"\n",
    "prompt = \"What do you see here?\"\n",
    "\n",
    "# Apply chat template\n",
    "formatted_prompt = apply_chat_template(\n",
    "    processor, config, prompt, num_images=1\n",
    ")\n",
    "\n",
    "# Generate output\n",
    "output = generate(model, processor, formatted_prompt, image_url, verbose=False)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and display the image\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "display(img)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
